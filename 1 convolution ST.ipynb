{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* one convolution notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorflow on 8889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/notebooks/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/notebooks/src'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! ls /home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already satisfied: cython in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python2.7/dist-packages (from keras)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from keras)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python2.7/dist-packages (from keras)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python2.7/dist-packages (from keras)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras cython h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Convolution2D, Dense, Activation, Flatten, merge, Conv2D\n",
    "from keras.layers import MaxPooling2D, Dropout, LocallyConnected2D, MaxPool2D, MaxPool1D\n",
    "from keras.models import Model, load_model,  Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "from keras.activations import relu, softmax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "#from lib.data_split import load_train_data, load_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing the data(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "'''\n",
    "This module takes a file path and loades pre_extracted features \n",
    "from leaf kaggle competiton. The input is the file path for\n",
    "train or test files. \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def load_train(filepath):\n",
    "\n",
    "    #loading\n",
    "    train_df = pd.read_csv(filepath)\n",
    "    id = train_df.pop('id')\n",
    "    target = train_df.pop('species')\n",
    "    \n",
    "    #scaling\n",
    "    target = LabelEncoder().fit(target).transform(target)\n",
    "    features = StandardScaler().fit(train_df).transform(train_df)\n",
    "    \n",
    "    return id,features,target\n",
    "    \n",
    "\n",
    "def load_test(filepath):\n",
    "    \n",
    "    #loading\n",
    "    test_df = pd.read_csv(filepath)\n",
    "    id = test_df.pop('id')\n",
    "    \n",
    "    #scaling\n",
    "    features = StandardScaler().fit(test_df).transform(test_df)\n",
    "    \n",
    "    return id, features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "'''\n",
    "This module loads all the training/test images, resizes them and \n",
    "makes them ready for kaggle.\n",
    "'''\n",
    "\n",
    "\n",
    "def rebuild_image(img, dim=64):\n",
    "    \n",
    "    # find largest axis\n",
    "    maxsize = max((0,1), key=lambda i: img.size[i])\n",
    "    \n",
    "    #scale both axis\n",
    "    scale = dim/float(img.size[maxsize])\n",
    "    \n",
    "    return img.resize((int(img.size[0] * scale), int(img.size[1]*scale)))\n",
    "    \n",
    "\n",
    "    \n",
    "def load_image(ids,dim=64):\n",
    "\n",
    "    filepath = './data/images/'\n",
    "    \n",
    "    IM = np.empty((len(ids),dim,dim,1))\n",
    "    \n",
    "    for i, ids in enumerate(ids):\n",
    "    \n",
    "        images = rebuild_image(load_img(filepath + str(ids) + '.jpg',\n",
    "grayscale=True),dim = dim)\n",
    "        \n",
    "        images = img_to_array(images)\n",
    "        x = images.shape[0]\n",
    "        y = images.shape[1]\n",
    "        \n",
    "        #centering\n",
    "        l_bound_0 = int((dim - x) / 2)\n",
    "        u_bound_0 = l_bound_0 + x\n",
    "        l_bound_1 = int((dim - y) / 2)\n",
    "        u_bound_1 = l_bound_1 + y\n",
    "        \n",
    "        IM[i,l_bound_0:u_bound_0,l_bound_1:u_bound_1,0:1] = images\n",
    "        \n",
    "        #scale it to grey\n",
    "    return np.around(IM/ 255.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASAAAAD8CAYAAADXCHlgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f70f4f00790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reading an image file using matplotlib into a numpy array\n",
    "# good ones: 11, 19, 23, 27, 48, 53, 78, 218\n",
    "img = mpimg.imread('./data/images/53.jpg')\n",
    "plt.imshow(img, cmap='Greens') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting the data with StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "random_state = 8\n",
    "split = 0.8\n",
    "\n",
    "\n",
    "\n",
    "def load_train_data(filepath, split = split, random_state = random_state):\n",
    "\n",
    "    id, features_train, target_train = load_train(filepath)\n",
    "    image_train = load_image(id)\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits = 1, train_size=split ,random_state=random_state)\n",
    "    train_i,val_i = next(sss.split(features_train,target_train))\n",
    "      \n",
    "    features_val,val_img,val_target = features_train[val_i],image_train[val_i], target_train[val_i]\n",
    "      \n",
    "    features_train,train_img,train_target = features_train[train_i],image_train[train_i],target_train[train_i]\n",
    "    \n",
    "    \n",
    "    return (features_train,train_img,train_target),(features_val,val_img,val_target)\n",
    "    \n",
    "\n",
    "def load_test_data(filepath):\n",
    "\n",
    "    id, test_features = load_test(filepath)\n",
    "    test_images = load_image(id)\n",
    "    \n",
    "    return (id,test_features,test_images)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_split.py:1630: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#X_train, y_train\n",
    "train,validation = load_train_data(\"./data/train.csv\",split = split, random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(792, 192)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = load_test_data(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = train[2][:]\n",
    "valid_labels = validation[2][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(792,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([78, 69, 33, 89, 92, 73, 75, 23, 17, 88, 69, 35, 97, 52, 87, 23, 18,\n",
       "       24, 70, 95, 80, 43, 22,  1, 84, 68, 34, 76, 36, 61, 32, 93, 95,  1,\n",
       "       55, 59, 33, 97, 22,  6, 28, 38, 50, 96, 85, 85, 82, 37, 47, 25, 41,\n",
       "       54, 33, 36, 70, 55, 42, 84, 63,  6, 51, 37,  6, 62, 43,  2, 30, 28,\n",
       "        2, 19, 74, 77, 14, 26, 21, 93,  7, 83, 56, 77, 62, 72,  3, 24, 89,\n",
       "       20, 71, 58,  4, 29, 68,  9, 33, 39, 30, 13, 92, 10, 81, 37, 61, 20,\n",
       "       89, 90,  3, 81, 79, 29, 80, 12, 34, 43, 37,  9, 16, 50, 74,  8, 60,\n",
       "       96, 13,  0, 69, 78, 10, 65, 47, 58, 92, 90,  5, 71, 30, 33, 56, 73,\n",
       "       21, 86, 53,  5, 15, 64, 16, 82, 62, 64, 47, 53, 30, 42, 45, 50, 63,\n",
       "       65, 14, 28, 44, 28, 75, 72, 82, 94, 39, 10, 78, 67, 20, 71, 64, 75,\n",
       "       44,  4, 92, 43, 47, 88, 46, 16, 25, 23,  0,  7,  6, 85, 35, 70,  7,\n",
       "       53,  9, 43, 59, 67, 59, 79, 68, 17, 11, 98, 77, 20, 31,  4, 21, 41,\n",
       "        8, 57, 24, 89, 87, 68, 74, 46,  2, 18, 92,  7, 51, 70, 68, 92, 90,\n",
       "       63, 12, 32, 98, 45, 80, 19, 54,  8, 15, 50, 48, 19, 76, 91, 74, 96,\n",
       "        1, 94, 11, 88, 10, 36, 35, 61,  2, 93, 33, 70, 91, 75, 38, 20, 13,\n",
       "       15, 12, 28, 48, 89, 34, 21, 44, 75, 51, 91, 59, 76, 50, 25, 35,  8,\n",
       "       86,  8, 40, 40, 66, 90, 37, 45, 14, 76,  0, 49, 54,  9, 19, 81, 18,\n",
       "       83, 83, 60, 32, 38, 33, 59,  8, 83,  3, 49,  5, 12, 86, 73, 76, 66,\n",
       "       97, 54,  6, 53, 82, 13, 26, 44,  7, 98, 21, 26, 74,  0, 22, 32, 81,\n",
       "       65, 19, 26, 92, 40, 22, 91, 10, 88, 13, 38, 19, 51, 80, 32,  5, 55,\n",
       "       53, 97, 98, 23, 64, 45, 30, 61, 60, 51, 30,  0, 37, 91, 15, 41, 55,\n",
       "       42, 46, 94, 13,  2, 81, 64, 83, 40, 61, 77, 94, 39, 18, 44, 58,  7,\n",
       "       68, 42, 34, 56, 77, 39, 25, 27, 50,  4, 75, 52, 25, 48, 47,  2, 86,\n",
       "       24, 56, 87, 82, 69,  3, 54, 55, 58, 71, 27, 51, 64,  1, 77, 44, 90,\n",
       "       32, 69, 48, 24, 85, 90, 88, 97, 41, 50, 18, 16, 54,  9, 31, 38,  3,\n",
       "       45, 23, 84, 48, 79, 41, 48, 78, 49, 71, 31,  4, 46, 76, 62, 88, 39,\n",
       "       89, 11, 11, 30, 56, 15, 37, 43, 80, 20, 75, 80, 49, 34,  7, 93, 36,\n",
       "       41, 68, 14, 42, 13, 22, 43, 18, 58, 34, 76, 96, 82, 66, 15, 29, 57,\n",
       "       71, 23, 17, 98, 29, 35, 27, 96, 83, 57, 79, 31, 65, 94, 24, 52, 10,\n",
       "       36, 32, 39, 84, 64, 49, 31, 35,  3, 98, 60, 14, 40, 52, 87, 88, 31,\n",
       "       79, 35, 60, 78, 46, 85, 37, 29,  1, 57, 48, 11,  4, 91, 55, 52, 96,\n",
       "       11, 49,  7, 96, 22,  0, 62, 66,  8,  9, 26, 16, 57,  5, 27, 58, 53,\n",
       "        5, 65, 36, 36, 17, 21, 66, 53, 28, 34, 87, 56, 95, 59, 22,  9, 21,\n",
       "       47, 16, 16, 24, 52, 63, 44, 80, 35, 12, 62, 54, 17, 45, 80, 25, 82,\n",
       "       19, 41, 46, 95, 12,  2, 79, 87, 31, 84, 78, 73,  8, 67, 62, 93, 31,\n",
       "       77, 40, 17, 72, 70, 39, 84, 24, 77, 64, 74, 63, 87, 13, 59, 92, 53,\n",
       "       96, 27, 69, 72, 52, 40, 56, 90, 97, 57, 23,  4,  3, 90, 42, 89, 55,\n",
       "       95, 72,  1, 22,  1, 12, 94, 95, 41, 55, 56, 62, 27, 91, 39, 18, 88,\n",
       "        6, 25,  9, 65, 38, 83, 57, 27, 97, 21,  1, 48, 74,  6,  5, 73, 16,\n",
       "       14, 10, 45, 84, 79, 63, 61, 60, 50,  2, 32, 83, 95, 17, 46, 58, 51,\n",
       "       65, 69, 78, 78, 17, 84, 67,  0, 26, 65, 59, 14, 68, 46, 86, 66, 81,\n",
       "       30, 72, 20, 60, 98, 49, 76, 47, 63, 60, 98, 26, 28, 67, 87, 73, 44,\n",
       "       70, 25, 81, 47,  3, 33, 23, 36, 94, 63, 49,  4,  6, 73, 15, 19, 74,\n",
       "       43, 27, 73, 85, 89, 28,  0, 93, 75, 70, 42, 71, 29, 93, 58, 18, 52,\n",
       "       14, 94, 54, 85, 61, 85, 51, 81, 11, 72, 66, 67, 34, 72, 91, 66, 67,\n",
       "       86, 61, 29, 97,  5, 82, 71, 86, 40, 42, 79, 93, 69, 67, 95, 45, 57,\n",
       "       86, 10, 26, 38, 11, 15, 12, 20, 38, 29])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99,)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([96,  4, 80,  8, 26,  7, 98, 38, 53, 72, 68, 58, 34, 60, 89,  9, 86,\n",
       "       54, 40,  6, 21, 49, 29, 75, 91, 27, 33, 11, 95, 57, 32, 35, 61, 19,\n",
       "       12, 44, 31, 41, 24, 94, 30, 18, 17,  1, 45, 20, 48, 10, 84, 67, 37,\n",
       "       81, 93, 59, 66, 23, 22, 64, 83, 46, 69, 25, 79, 50, 47, 90, 51,  3,\n",
       "       15,  2, 39, 73,  5, 63, 97, 82, 92, 85, 74, 36, 71, 13, 14, 70, 88,\n",
       "       55, 28, 16, 56,  0, 43, 65, 76, 42, 52, 78, 77, 62, 87])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reformat function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reformat(labels):\n",
    "    labels = (np.arange(99) == labels[:,None]).astype(np.float32)\n",
    "    return labels\n",
    "train_labels = reformat(train_labels)\n",
    "valid_labels = reformat(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(792, 99)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 99)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(64,64,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_1:0' shape=(?, 64, 64, 1) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(64), Dimension(64), Dimension(1)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frankenstein Neural Network (FNN)\n",
    "* Neural Network the functional way ( concating images and features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first network only for the images\n",
    "\n",
    "Input_layer = Input(shape=(64,64,1))\n",
    "\n",
    "first = Conv2D(8, kernel_size=(3,3), activation= \"relu\", padding= \"same\")(Input_layer)\n",
    "\n",
    "first = Conv2D(16, kernel_size=(3,3), activation = \"relu\")(first)\n",
    "\n",
    "first = Dropout(.5)(first)\n",
    "\n",
    "first = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(first)\n",
    "\n",
    "first = Flatten()(first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second network only for the pre_extracted features\n",
    "\n",
    "Input_layer_features = Input(shape=(192,1),name='second')\n",
    "\n",
    "second = Dense(100, activation = \"relu\")(Input_layer_features)\n",
    "\n",
    "second = MaxPool1D()(second)\n",
    "\n",
    "second = Flatten()(second)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import concatenate#,Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concating the first network(the images only) and the second network(the pre_extracted features only)\n",
    "combined = concatenate([first,second])\n",
    "type(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined = Dense(100, activation= \"relu\")(combined)\n",
    "combined = Dropout(.5)(combined)\n",
    "#output layer\n",
    "output_layer = Dense(99, activation=\"softmax\")(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_24/Softmax:0' shape=(?, 99) dtype=float32>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 64, 64, 1)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation[1][:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 192)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation[0][:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Model(inputs=[Input_layer, Input_layer_features],outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#every time you compile your model It will start over the scoring\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[1][:].shape\n",
    "# train[0][:].reshape((792,192,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reshaping from 2D to 3D\n",
    "train_features = train[0][:].reshape((792,192,1))\n",
    "validation_features = validation[0][:].reshape((99,192,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 792 samples, validate on 99 samples\n",
      "Epoch 1/20\n",
      "792/792 [==============================] - 4s - loss: 4.5734 - acc: 0.0253 - val_loss: 4.1950 - val_acc: 0.1010\n",
      "Epoch 2/20\n",
      "792/792 [==============================] - 3s - loss: 3.9926 - acc: 0.0833 - val_loss: 3.5116 - val_acc: 0.2828\n",
      "Epoch 3/20\n",
      "792/792 [==============================] - 3s - loss: 3.2344 - acc: 0.2210 - val_loss: 2.6663 - val_acc: 0.4848\n",
      "Epoch 4/20\n",
      "792/792 [==============================] - 3s - loss: 2.5039 - acc: 0.3396 - val_loss: 1.8483 - val_acc: 0.7879\n",
      "Epoch 5/20\n",
      "792/792 [==============================] - 3s - loss: 1.8919 - acc: 0.4722 - val_loss: 1.1577 - val_acc: 0.9293\n",
      "Epoch 6/20\n",
      "792/792 [==============================] - 4s - loss: 1.4627 - acc: 0.5783 - val_loss: 0.6830 - val_acc: 0.9394\n",
      "Epoch 7/20\n",
      "792/792 [==============================] - 4s - loss: 1.1697 - acc: 0.6616 - val_loss: 0.5069 - val_acc: 0.9697\n",
      "Epoch 8/20\n",
      "792/792 [==============================] - 4s - loss: 0.9239 - acc: 0.7273 - val_loss: 0.3615 - val_acc: 0.9899\n",
      "Epoch 9/20\n",
      "792/792 [==============================] - 4s - loss: 0.7991 - acc: 0.7538 - val_loss: 0.2410 - val_acc: 0.9798\n",
      "Epoch 10/20\n",
      "792/792 [==============================] - 4s - loss: 0.6740 - acc: 0.7790 - val_loss: 0.2146 - val_acc: 0.9899\n",
      "Epoch 11/20\n",
      "792/792 [==============================] - 4s - loss: 0.6643 - acc: 0.8018 - val_loss: 0.1678 - val_acc: 0.9899\n",
      "Epoch 12/20\n",
      "792/792 [==============================] - 4s - loss: 0.6467 - acc: 0.8093 - val_loss: 0.1675 - val_acc: 0.9899\n",
      "Epoch 13/20\n",
      "792/792 [==============================] - 4s - loss: 0.5080 - acc: 0.8485 - val_loss: 0.1409 - val_acc: 0.9899\n",
      "Epoch 14/20\n",
      "792/792 [==============================] - 4s - loss: 0.4800 - acc: 0.8460 - val_loss: 0.1456 - val_acc: 0.9495\n",
      "Epoch 15/20\n",
      "792/792 [==============================] - 4s - loss: 0.5436 - acc: 0.8321 - val_loss: 0.1511 - val_acc: 0.9798\n",
      "Epoch 16/20\n",
      "792/792 [==============================] - 4s - loss: 0.4337 - acc: 0.8725 - val_loss: 0.1018 - val_acc: 0.9798\n",
      "Epoch 17/20\n",
      "792/792 [==============================] - 4s - loss: 0.4159 - acc: 0.8535 - val_loss: 0.1191 - val_acc: 0.9697\n",
      "Epoch 18/20\n",
      "792/792 [==============================] - 4s - loss: 0.3777 - acc: 0.8838 - val_loss: 0.0948 - val_acc: 1.0000\n",
      "Epoch 19/20\n",
      "792/792 [==============================] - 4s - loss: 0.3567 - acc: 0.8763 - val_loss: 0.0991 - val_acc: 0.9697\n",
      "Epoch 20/20\n",
      "792/792 [==============================] - 4s - loss: 0.3280 - acc: 0.8952 - val_loss: 0.0631 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f70c728d7d0>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([train[1][:],train_features], train_labels, validation_data = ([validation[1][:],validation_features],valid_labels),epochs= 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Sequential Convolutional Neural Network(CNN)-images only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#start with the images only.\n",
    "\n",
    "cnn = Sequential([\n",
    "    Conv2D(8, kernel_size=(3,3),\n",
    "          activation= \"relu\",\n",
    "          input_shape = (64,64,1)),\n",
    "    Conv2D(16, kernel_size=(3,3), \n",
    "           activation = \"relu\"),\n",
    "    Dropout(.25),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(100, activation = \"relu\"),\n",
    "    Dropout(.25),\n",
    "    #output _ layer\n",
    "    Dense(256),\n",
    "    Dense(99, activation= \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_23 (Conv2D)           (None, 62, 62, 8)         80        \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 60, 60, 16)        1168      \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 60, 60, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 14400)             0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 100)               1440100   \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 99)                25443     \n",
      "=================================================================\n",
      "Total params: 1,492,647\n",
      "Trainable params: 1,492,647\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn.compile(loss= keras.losses.categorical_crossentropy,\n",
    "           optimizer= keras.optimizers.adam(),\n",
    "           metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 792 samples, validate on 99 samples\n",
      "Epoch 1/20\n",
      "792/792 [==============================] - 3s - loss: 4.5576 - acc: 0.0202 - val_loss: 4.2439 - val_acc: 0.0808\n",
      "Epoch 2/20\n",
      "792/792 [==============================] - 3s - loss: 3.7604 - acc: 0.0846 - val_loss: 3.1458 - val_acc: 0.1818\n",
      "Epoch 3/20\n",
      "792/792 [==============================] - 3s - loss: 2.8101 - acc: 0.2449 - val_loss: 2.4668 - val_acc: 0.3535\n",
      "Epoch 4/20\n",
      "792/792 [==============================] - 3s - loss: 2.2514 - acc: 0.3245 - val_loss: 2.1048 - val_acc: 0.4242\n",
      "Epoch 5/20\n",
      "792/792 [==============================] - 3s - loss: 1.8565 - acc: 0.4571 - val_loss: 1.8336 - val_acc: 0.5152\n",
      "Epoch 6/20\n",
      "792/792 [==============================] - 3s - loss: 1.7217 - acc: 0.4722 - val_loss: 1.7084 - val_acc: 0.5354\n",
      "Epoch 7/20\n",
      "792/792 [==============================] - 3s - loss: 1.4511 - acc: 0.5682 - val_loss: 1.5166 - val_acc: 0.5354\n",
      "Epoch 8/20\n",
      "792/792 [==============================] - 3s - loss: 1.2963 - acc: 0.5960 - val_loss: 1.5006 - val_acc: 0.5758\n",
      "Epoch 9/20\n",
      "792/792 [==============================] - 3s - loss: 1.1555 - acc: 0.6288 - val_loss: 1.5559 - val_acc: 0.5253\n",
      "Epoch 10/20\n",
      "792/792 [==============================] - 3s - loss: 1.0568 - acc: 0.6641 - val_loss: 1.5516 - val_acc: 0.5657\n",
      "Epoch 11/20\n",
      "792/792 [==============================] - 3s - loss: 0.9591 - acc: 0.6667 - val_loss: 1.5054 - val_acc: 0.5859\n",
      "Epoch 12/20\n",
      "792/792 [==============================] - 3s - loss: 0.9013 - acc: 0.7247 - val_loss: 1.4944 - val_acc: 0.5859\n",
      "Epoch 13/20\n",
      "792/792 [==============================] - 3s - loss: 0.8083 - acc: 0.7348 - val_loss: 1.4965 - val_acc: 0.5657\n",
      "Epoch 14/20\n",
      "792/792 [==============================] - 3s - loss: 0.7669 - acc: 0.7614 - val_loss: 1.4547 - val_acc: 0.5859\n",
      "Epoch 15/20\n",
      "792/792 [==============================] - 3s - loss: 0.6599 - acc: 0.7929 - val_loss: 1.5280 - val_acc: 0.5859\n",
      "Epoch 16/20\n",
      "792/792 [==============================] - 3s - loss: 0.6646 - acc: 0.7740 - val_loss: 1.4396 - val_acc: 0.5758\n",
      "Epoch 17/20\n",
      "792/792 [==============================] - 3s - loss: 0.6071 - acc: 0.7942 - val_loss: 1.4575 - val_acc: 0.5556\n",
      "Epoch 18/20\n",
      "792/792 [==============================] - 3s - loss: 0.5863 - acc: 0.8144 - val_loss: 1.5812 - val_acc: 0.5556\n",
      "Epoch 19/20\n",
      "792/792 [==============================] - 3s - loss: 0.5764 - acc: 0.7980 - val_loss: 1.7092 - val_acc: 0.5455\n",
      "Epoch 20/20\n",
      "792/792 [==============================] - 3s - loss: 0.4433 - acc: 0.8548 - val_loss: 1.6206 - val_acc: 0.5354\n"
     ]
    }
   ],
   "source": [
    "# for images only ==> train[1] and validation[1]\n",
    "history = cnn.fit(train[1],train_labels, validation_data=(validation[1], valid_labels),nb_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(792, 64, 64, 1)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[1][:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(792, 192)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#you need an optimizer anyway.\n",
    "#sgd = SGD(lr=0.008, momentum=0.9,decay=0.0,nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=[<tf.Tenso...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# for image + numerical data\n",
    "model = Model(input=[input_layer,num],output=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.compile(optimizer='Adam',loss='categorical_crossentropy',\n",
    "#              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install seaborn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
