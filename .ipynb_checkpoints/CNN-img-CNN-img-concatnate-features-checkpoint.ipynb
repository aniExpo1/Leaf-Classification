{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* one convolution notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorflow on 8889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/notebooks/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/notebooks/src'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! ls /home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already satisfied: cython in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python2.7/dist-packages (from keras)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from keras)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python2.7/dist-packages (from keras)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python2.7/dist-packages (from keras)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras cython h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Convolution2D, Dense, Activation, Flatten, merge, Conv2D\n",
    "from keras.layers import MaxPooling2D, Dropout, LocallyConnected2D, MaxPool2D, MaxPool1D\n",
    "from keras.models import Model, load_model,  Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "from keras.activations import relu, softmax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "#from lib.data_split import load_train_data, load_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing the data(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "'''\n",
    "This module takes a file path and loades pre_extracted features \n",
    "from leaf kaggle competiton. The input is the file path for\n",
    "train or test files. \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def load_train(filepath):\n",
    "\n",
    "    #loading\n",
    "    train_df = pd.read_csv(filepath)\n",
    "    id = train_df.pop('id')\n",
    "    target = train_df.pop('species')\n",
    "    \n",
    "    #scaling\n",
    "    target = LabelEncoder().fit(target).transform(target)\n",
    "    features = StandardScaler().fit(train_df).transform(train_df)\n",
    "    \n",
    "    return id,features,target\n",
    "    \n",
    "\n",
    "def load_test(filepath):\n",
    "    \n",
    "    #loading\n",
    "    test_df = pd.read_csv(filepath)\n",
    "    id = test_df.pop('id')\n",
    "    \n",
    "    #scaling\n",
    "    features = StandardScaler().fit(test_df).transform(test_df)\n",
    "    \n",
    "    return id, features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "'''\n",
    "This module loads all the training/test images, resizes them and \n",
    "makes them ready for kaggle.\n",
    "'''\n",
    "\n",
    "\n",
    "def rebuild_image(img, dim=64):\n",
    "    \n",
    "    # find largest axis\n",
    "    maxsize = max((0,1), key=lambda i: img.size[i])\n",
    "    \n",
    "    #scale both axis\n",
    "    scale = dim/float(img.size[maxsize])\n",
    "    \n",
    "    return img.resize((int(img.size[0] * scale), int(img.size[1]*scale)))\n",
    "    \n",
    "\n",
    "    \n",
    "def load_image(ids,dim=64):\n",
    "\n",
    "    filepath = './data/images/'\n",
    "    \n",
    "    IM = np.empty((len(ids),dim,dim,1))\n",
    "    \n",
    "    for i, ids in enumerate(ids):\n",
    "    \n",
    "        images = rebuild_image(load_img(filepath + str(ids) + '.jpg',\n",
    "grayscale=True),dim = dim)\n",
    "        \n",
    "        images = img_to_array(images)\n",
    "        x = images.shape[0]\n",
    "        y = images.shape[1]\n",
    "        \n",
    "        #centering\n",
    "        l_bound_0 = int((dim - x) / 2)\n",
    "        u_bound_0 = l_bound_0 + x\n",
    "        l_bound_1 = int((dim - y) / 2)\n",
    "        u_bound_1 = l_bound_1 + y\n",
    "        \n",
    "        IM[i,l_bound_0:u_bound_0,l_bound_1:u_bound_1,0:1] = images\n",
    "        \n",
    "        #scale it to grey\n",
    "    return np.around(IM/ 255.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASAAAAD8CAYAAADXCHlgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f70f4f00790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reading an image file using matplotlib into a numpy array\n",
    "# good ones: 11, 19, 23, 27, 48, 53, 78, 218\n",
    "img = mpimg.imread('./data/images/53.jpg')\n",
    "plt.imshow(img, cmap='Greens') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting the data with StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "random_state = 8\n",
    "split = 0.8\n",
    "\n",
    "\n",
    "\n",
    "def load_train_data(filepath, split = split, random_state = random_state):\n",
    "\n",
    "    id, features_train, target_train = load_train(filepath)\n",
    "    image_train = load_image(id)\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits = 1, train_size=split ,random_state=random_state)\n",
    "    train_i,val_i = next(sss.split(features_train,target_train))\n",
    "      \n",
    "    features_val,val_img,val_target = features_train[val_i],image_train[val_i], target_train[val_i]\n",
    "      \n",
    "    features_train,train_img,train_target = features_train[train_i],image_train[train_i],target_train[train_i]\n",
    "    \n",
    "    \n",
    "    return (features_train,train_img,train_target),(features_val,val_img,val_target)\n",
    "    \n",
    "\n",
    "def load_test_data(filepath):\n",
    "\n",
    "    id, test_features = load_test(filepath)\n",
    "    test_images = load_image(id)\n",
    "    \n",
    "    return (id,test_features,test_images)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_split.py:1630: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#X_train, y_train\n",
    "train,validation = load_train_data(\"./data/train.csv\",split = split, random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(792, 192)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = load_test_data(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = train[2][:]\n",
    "valid_labels = validation[2][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(792,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([78, 69, 33, 89, 92, 73, 75, 23, 17, 88, 69, 35, 97, 52, 87, 23, 18,\n",
       "       24, 70, 95, 80, 43, 22,  1, 84, 68, 34, 76, 36, 61, 32, 93, 95,  1,\n",
       "       55, 59, 33, 97, 22,  6, 28, 38, 50, 96, 85, 85, 82, 37, 47, 25, 41,\n",
       "       54, 33, 36, 70, 55, 42, 84, 63,  6, 51, 37,  6, 62, 43,  2, 30, 28,\n",
       "        2, 19, 74, 77, 14, 26, 21, 93,  7, 83, 56, 77, 62, 72,  3, 24, 89,\n",
       "       20, 71, 58,  4, 29, 68,  9, 33, 39, 30, 13, 92, 10, 81, 37, 61, 20,\n",
       "       89, 90,  3, 81, 79, 29, 80, 12, 34, 43, 37,  9, 16, 50, 74,  8, 60,\n",
       "       96, 13,  0, 69, 78, 10, 65, 47, 58, 92, 90,  5, 71, 30, 33, 56, 73,\n",
       "       21, 86, 53,  5, 15, 64, 16, 82, 62, 64, 47, 53, 30, 42, 45, 50, 63,\n",
       "       65, 14, 28, 44, 28, 75, 72, 82, 94, 39, 10, 78, 67, 20, 71, 64, 75,\n",
       "       44,  4, 92, 43, 47, 88, 46, 16, 25, 23,  0,  7,  6, 85, 35, 70,  7,\n",
       "       53,  9, 43, 59, 67, 59, 79, 68, 17, 11, 98, 77, 20, 31,  4, 21, 41,\n",
       "        8, 57, 24, 89, 87, 68, 74, 46,  2, 18, 92,  7, 51, 70, 68, 92, 90,\n",
       "       63, 12, 32, 98, 45, 80, 19, 54,  8, 15, 50, 48, 19, 76, 91, 74, 96,\n",
       "        1, 94, 11, 88, 10, 36, 35, 61,  2, 93, 33, 70, 91, 75, 38, 20, 13,\n",
       "       15, 12, 28, 48, 89, 34, 21, 44, 75, 51, 91, 59, 76, 50, 25, 35,  8,\n",
       "       86,  8, 40, 40, 66, 90, 37, 45, 14, 76,  0, 49, 54,  9, 19, 81, 18,\n",
       "       83, 83, 60, 32, 38, 33, 59,  8, 83,  3, 49,  5, 12, 86, 73, 76, 66,\n",
       "       97, 54,  6, 53, 82, 13, 26, 44,  7, 98, 21, 26, 74,  0, 22, 32, 81,\n",
       "       65, 19, 26, 92, 40, 22, 91, 10, 88, 13, 38, 19, 51, 80, 32,  5, 55,\n",
       "       53, 97, 98, 23, 64, 45, 30, 61, 60, 51, 30,  0, 37, 91, 15, 41, 55,\n",
       "       42, 46, 94, 13,  2, 81, 64, 83, 40, 61, 77, 94, 39, 18, 44, 58,  7,\n",
       "       68, 42, 34, 56, 77, 39, 25, 27, 50,  4, 75, 52, 25, 48, 47,  2, 86,\n",
       "       24, 56, 87, 82, 69,  3, 54, 55, 58, 71, 27, 51, 64,  1, 77, 44, 90,\n",
       "       32, 69, 48, 24, 85, 90, 88, 97, 41, 50, 18, 16, 54,  9, 31, 38,  3,\n",
       "       45, 23, 84, 48, 79, 41, 48, 78, 49, 71, 31,  4, 46, 76, 62, 88, 39,\n",
       "       89, 11, 11, 30, 56, 15, 37, 43, 80, 20, 75, 80, 49, 34,  7, 93, 36,\n",
       "       41, 68, 14, 42, 13, 22, 43, 18, 58, 34, 76, 96, 82, 66, 15, 29, 57,\n",
       "       71, 23, 17, 98, 29, 35, 27, 96, 83, 57, 79, 31, 65, 94, 24, 52, 10,\n",
       "       36, 32, 39, 84, 64, 49, 31, 35,  3, 98, 60, 14, 40, 52, 87, 88, 31,\n",
       "       79, 35, 60, 78, 46, 85, 37, 29,  1, 57, 48, 11,  4, 91, 55, 52, 96,\n",
       "       11, 49,  7, 96, 22,  0, 62, 66,  8,  9, 26, 16, 57,  5, 27, 58, 53,\n",
       "        5, 65, 36, 36, 17, 21, 66, 53, 28, 34, 87, 56, 95, 59, 22,  9, 21,\n",
       "       47, 16, 16, 24, 52, 63, 44, 80, 35, 12, 62, 54, 17, 45, 80, 25, 82,\n",
       "       19, 41, 46, 95, 12,  2, 79, 87, 31, 84, 78, 73,  8, 67, 62, 93, 31,\n",
       "       77, 40, 17, 72, 70, 39, 84, 24, 77, 64, 74, 63, 87, 13, 59, 92, 53,\n",
       "       96, 27, 69, 72, 52, 40, 56, 90, 97, 57, 23,  4,  3, 90, 42, 89, 55,\n",
       "       95, 72,  1, 22,  1, 12, 94, 95, 41, 55, 56, 62, 27, 91, 39, 18, 88,\n",
       "        6, 25,  9, 65, 38, 83, 57, 27, 97, 21,  1, 48, 74,  6,  5, 73, 16,\n",
       "       14, 10, 45, 84, 79, 63, 61, 60, 50,  2, 32, 83, 95, 17, 46, 58, 51,\n",
       "       65, 69, 78, 78, 17, 84, 67,  0, 26, 65, 59, 14, 68, 46, 86, 66, 81,\n",
       "       30, 72, 20, 60, 98, 49, 76, 47, 63, 60, 98, 26, 28, 67, 87, 73, 44,\n",
       "       70, 25, 81, 47,  3, 33, 23, 36, 94, 63, 49,  4,  6, 73, 15, 19, 74,\n",
       "       43, 27, 73, 85, 89, 28,  0, 93, 75, 70, 42, 71, 29, 93, 58, 18, 52,\n",
       "       14, 94, 54, 85, 61, 85, 51, 81, 11, 72, 66, 67, 34, 72, 91, 66, 67,\n",
       "       86, 61, 29, 97,  5, 82, 71, 86, 40, 42, 79, 93, 69, 67, 95, 45, 57,\n",
       "       86, 10, 26, 38, 11, 15, 12, 20, 38, 29])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99,)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([96,  4, 80,  8, 26,  7, 98, 38, 53, 72, 68, 58, 34, 60, 89,  9, 86,\n",
       "       54, 40,  6, 21, 49, 29, 75, 91, 27, 33, 11, 95, 57, 32, 35, 61, 19,\n",
       "       12, 44, 31, 41, 24, 94, 30, 18, 17,  1, 45, 20, 48, 10, 84, 67, 37,\n",
       "       81, 93, 59, 66, 23, 22, 64, 83, 46, 69, 25, 79, 50, 47, 90, 51,  3,\n",
       "       15,  2, 39, 73,  5, 63, 97, 82, 92, 85, 74, 36, 71, 13, 14, 70, 88,\n",
       "       55, 28, 16, 56,  0, 43, 65, 76, 42, 52, 78, 77, 62, 87])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reformat function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reformat(labels):\n",
    "    labels = (np.arange(99) == labels[:,None]).astype(np.float32)\n",
    "    return labels\n",
    "train_labels = reformat(train_labels)\n",
    "valid_labels = reformat(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(792, 99)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 99)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(64,64,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_1:0' shape=(?, 64, 64, 1) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(64), Dimension(64), Dimension(1)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frankenstein Neural Network (FNN)\n",
    "* Neural Network the functional way ( concating images and features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first network only for the images\n",
    "\n",
    "Input_layer = Input(shape=(64,64,1))\n",
    "\n",
    "first = Conv2D(8, kernel_size=(3,3), activation= \"relu\", padding= \"same\")(Input_layer)\n",
    "\n",
    "first = Conv2D(16, kernel_size=(3,3), activation = \"relu\")(first)\n",
    "\n",
    "first = Dropout(.5)(first)\n",
    "\n",
    "first = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(first)\n",
    "\n",
    "first = Flatten()(first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second network only for the pre_extracted features\n",
    "\n",
    "Input_layer_features = Input(shape=(192,1),name='second')\n",
    "\n",
    "second = Dense(100, activation = \"relu\")(Input_layer_features)\n",
    "\n",
    "second = MaxPool1D()(second)\n",
    "\n",
    "second = Flatten()(second)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import concatenate#,Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concating the first network(the images only) and the second network(the pre_extracted features only)\n",
    "combined = concatenate([first,second])\n",
    "type(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined = Dense(100, activation= \"relu\")(combined)\n",
    "combined = Dropout(.5)(combined)\n",
    "#output layer\n",
    "output_layer = Dense(99, activation=\"softmax\")(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_27/Softmax:0' shape=(?, 99) dtype=float32>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 64, 64, 1)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation[1][:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 192)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation[0][:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Model(inputs=[Input_layer, Input_layer_features],outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#every time you compile your model It will start over the scoring\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[1][:].shape\n",
    "# train[0][:].reshape((792,192,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reshaping from 2D to 3D\n",
    "train_features = train[0][:].reshape((792,192,1))\n",
    "validation_features = validation[0][:].reshape((99,192,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 792 samples, validate on 99 samples\n",
      "Epoch 1/100\n",
      "792/792 [==============================] - 4s - loss: 4.5000 - acc: 0.0417 - val_loss: 4.0854 - val_acc: 0.1616\n",
      "Epoch 2/100\n",
      "792/792 [==============================] - 3s - loss: 3.8691 - acc: 0.1212 - val_loss: 3.3598 - val_acc: 0.3131\n",
      "Epoch 3/100\n",
      "792/792 [==============================] - 4s - loss: 3.2117 - acc: 0.1982 - val_loss: 2.5411 - val_acc: 0.5657\n",
      "Epoch 4/100\n",
      "792/792 [==============================] - 3s - loss: 2.4833 - acc: 0.3497 - val_loss: 1.7373 - val_acc: 0.6869\n",
      "Epoch 5/100\n",
      "792/792 [==============================] - 3s - loss: 1.9061 - acc: 0.4760 - val_loss: 1.1825 - val_acc: 0.8687\n",
      "Epoch 6/100\n",
      "792/792 [==============================] - 3s - loss: 1.5203 - acc: 0.5644 - val_loss: 0.8222 - val_acc: 0.8990\n",
      "Epoch 7/100\n",
      "792/792 [==============================] - 3s - loss: 1.2558 - acc: 0.6528 - val_loss: 0.5485 - val_acc: 0.9192\n",
      "Epoch 8/100\n",
      "792/792 [==============================] - 3s - loss: 1.1060 - acc: 0.6717 - val_loss: 0.4802 - val_acc: 0.9293\n",
      "Epoch 9/100\n",
      "792/792 [==============================] - 3s - loss: 0.9288 - acc: 0.7096 - val_loss: 0.3966 - val_acc: 0.9293\n",
      "Epoch 10/100\n",
      "792/792 [==============================] - 4s - loss: 0.8780 - acc: 0.7134 - val_loss: 0.3064 - val_acc: 0.9495\n",
      "Epoch 11/100\n",
      "792/792 [==============================] - 3s - loss: 0.7436 - acc: 0.7551 - val_loss: 0.2585 - val_acc: 0.9596\n",
      "Epoch 12/100\n",
      "792/792 [==============================] - 4s - loss: 0.6327 - acc: 0.8081 - val_loss: 0.2654 - val_acc: 0.9596\n",
      "Epoch 13/100\n",
      "792/792 [==============================] - 4s - loss: 0.6668 - acc: 0.7955 - val_loss: 0.2023 - val_acc: 0.9899\n",
      "Epoch 14/100\n",
      "792/792 [==============================] - 4s - loss: 0.5620 - acc: 0.8220 - val_loss: 0.2387 - val_acc: 0.9293\n",
      "Epoch 15/100\n",
      "792/792 [==============================] - 4s - loss: 0.5393 - acc: 0.8359 - val_loss: 0.1797 - val_acc: 0.9899\n",
      "Epoch 16/100\n",
      "792/792 [==============================] - 4s - loss: 0.5757 - acc: 0.8144 - val_loss: 0.1831 - val_acc: 0.9596\n",
      "Epoch 17/100\n",
      "792/792 [==============================] - 3s - loss: 0.4514 - acc: 0.8497 - val_loss: 0.1789 - val_acc: 0.9697\n",
      "Epoch 18/100\n",
      "792/792 [==============================] - 3s - loss: 0.4474 - acc: 0.8699 - val_loss: 0.1530 - val_acc: 0.9596\n",
      "Epoch 19/100\n",
      "792/792 [==============================] - 4s - loss: 0.4283 - acc: 0.8598 - val_loss: 0.1660 - val_acc: 0.9798\n",
      "Epoch 20/100\n",
      "792/792 [==============================] - 3s - loss: 0.4696 - acc: 0.8497 - val_loss: 0.1255 - val_acc: 0.9697\n",
      "Epoch 21/100\n",
      "792/792 [==============================] - 4s - loss: 0.4699 - acc: 0.8598 - val_loss: 0.1189 - val_acc: 0.9899\n",
      "Epoch 22/100\n",
      "792/792 [==============================] - 3s - loss: 0.3283 - acc: 0.8889 - val_loss: 0.1092 - val_acc: 0.9697\n",
      "Epoch 23/100\n",
      "792/792 [==============================] - 3s - loss: 0.3997 - acc: 0.8763 - val_loss: 0.1242 - val_acc: 0.9798\n",
      "Epoch 24/100\n",
      "792/792 [==============================] - 3s - loss: 0.3285 - acc: 0.8927 - val_loss: 0.1122 - val_acc: 0.9899\n",
      "Epoch 25/100\n",
      "792/792 [==============================] - 4s - loss: 0.3252 - acc: 0.8952 - val_loss: 0.0924 - val_acc: 0.9697\n",
      "Epoch 26/100\n",
      "792/792 [==============================] - 3s - loss: 0.3124 - acc: 0.8990 - val_loss: 0.0978 - val_acc: 0.9899\n",
      "Epoch 27/100\n",
      "792/792 [==============================] - 3s - loss: 0.3511 - acc: 0.8838 - val_loss: 0.1262 - val_acc: 0.9798\n",
      "Epoch 28/100\n",
      "792/792 [==============================] - 4s - loss: 0.3527 - acc: 0.8851 - val_loss: 0.1163 - val_acc: 0.9697\n",
      "Epoch 29/100\n",
      "792/792 [==============================] - 4s - loss: 0.3432 - acc: 0.8876 - val_loss: 0.0843 - val_acc: 0.9798\n",
      "Epoch 30/100\n",
      "792/792 [==============================] - 3s - loss: 0.3142 - acc: 0.8927 - val_loss: 0.0839 - val_acc: 0.9899\n",
      "Epoch 31/100\n",
      "792/792 [==============================] - 3s - loss: 0.3094 - acc: 0.8927 - val_loss: 0.0947 - val_acc: 0.9798\n",
      "Epoch 32/100\n",
      "792/792 [==============================] - 3s - loss: 0.3445 - acc: 0.8763 - val_loss: 0.1304 - val_acc: 0.9596\n",
      "Epoch 33/100\n",
      "792/792 [==============================] - 3s - loss: 0.2509 - acc: 0.9091 - val_loss: 0.0899 - val_acc: 0.9798\n",
      "Epoch 34/100\n",
      "792/792 [==============================] - 3s - loss: 0.2797 - acc: 0.9167 - val_loss: 0.0660 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "792/792 [==============================] - 3s - loss: 0.2528 - acc: 0.9066 - val_loss: 0.0740 - val_acc: 0.9798\n",
      "Epoch 36/100\n",
      "792/792 [==============================] - 3s - loss: 0.2825 - acc: 0.9104 - val_loss: 0.0660 - val_acc: 0.9798\n",
      "Epoch 37/100\n",
      "792/792 [==============================] - 3s - loss: 0.2963 - acc: 0.8977 - val_loss: 0.1160 - val_acc: 0.9697\n",
      "Epoch 38/100\n",
      "792/792 [==============================] - 4s - loss: 0.2384 - acc: 0.9192 - val_loss: 0.0715 - val_acc: 0.9798\n",
      "Epoch 39/100\n",
      "792/792 [==============================] - 3s - loss: 0.2848 - acc: 0.8965 - val_loss: 0.0786 - val_acc: 0.9899\n",
      "Epoch 40/100\n",
      "792/792 [==============================] - 3s - loss: 0.2601 - acc: 0.9091 - val_loss: 0.0917 - val_acc: 0.9798\n",
      "Epoch 41/100\n",
      "792/792 [==============================] - 4s - loss: 0.1842 - acc: 0.9394 - val_loss: 0.0948 - val_acc: 0.9697\n",
      "Epoch 42/100\n",
      "792/792 [==============================] - 4s - loss: 0.2644 - acc: 0.9141 - val_loss: 0.0695 - val_acc: 0.9798\n",
      "Epoch 43/100\n",
      "792/792 [==============================] - 3s - loss: 0.2488 - acc: 0.9040 - val_loss: 0.0926 - val_acc: 0.9798\n",
      "Epoch 44/100\n",
      "792/792 [==============================] - 3s - loss: 0.2332 - acc: 0.9230 - val_loss: 0.0721 - val_acc: 0.9798\n",
      "Epoch 45/100\n",
      "792/792 [==============================] - 4s - loss: 0.2230 - acc: 0.9293 - val_loss: 0.0634 - val_acc: 0.9798\n",
      "Epoch 46/100\n",
      "792/792 [==============================] - 3s - loss: 0.1858 - acc: 0.9356 - val_loss: 0.0705 - val_acc: 0.9697\n",
      "Epoch 47/100\n",
      "792/792 [==============================] - 3s - loss: 0.2619 - acc: 0.9116 - val_loss: 0.0572 - val_acc: 0.9697\n",
      "Epoch 48/100\n",
      "792/792 [==============================] - 4s - loss: 0.2279 - acc: 0.9242 - val_loss: 0.0456 - val_acc: 0.9798\n",
      "Epoch 49/100\n",
      "792/792 [==============================] - 4s - loss: 0.2360 - acc: 0.9217 - val_loss: 0.0661 - val_acc: 0.9697\n",
      "Epoch 50/100\n",
      "792/792 [==============================] - 4s - loss: 0.2541 - acc: 0.9141 - val_loss: 0.0327 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "792/792 [==============================] - 3s - loss: 0.2325 - acc: 0.9192 - val_loss: 0.0783 - val_acc: 0.9899\n",
      "Epoch 52/100\n",
      "792/792 [==============================] - 4s - loss: 0.2233 - acc: 0.9255 - val_loss: 0.0615 - val_acc: 0.9899\n",
      "Epoch 53/100\n",
      "792/792 [==============================] - 4s - loss: 0.2341 - acc: 0.9205 - val_loss: 0.0693 - val_acc: 0.9899\n",
      "Epoch 54/100\n",
      "792/792 [==============================] - 4s - loss: 0.1855 - acc: 0.9381 - val_loss: 0.0447 - val_acc: 0.9798\n",
      "Epoch 55/100\n",
      "792/792 [==============================] - 5s - loss: 0.2409 - acc: 0.9167 - val_loss: 0.0625 - val_acc: 0.9697\n",
      "Epoch 56/100\n",
      "792/792 [==============================] - 5s - loss: 0.2441 - acc: 0.9179 - val_loss: 0.0640 - val_acc: 0.9697\n",
      "Epoch 57/100\n",
      "792/792 [==============================] - 3s - loss: 0.2018 - acc: 0.9280 - val_loss: 0.0890 - val_acc: 0.9596\n",
      "Epoch 58/100\n",
      "792/792 [==============================] - 4s - loss: 0.2186 - acc: 0.9268 - val_loss: 0.0543 - val_acc: 0.9899\n",
      "Epoch 59/100\n",
      "792/792 [==============================] - 4s - loss: 0.2420 - acc: 0.9154 - val_loss: 0.0584 - val_acc: 0.9596\n",
      "Epoch 60/100\n",
      "792/792 [==============================] - 4s - loss: 0.1683 - acc: 0.9394 - val_loss: 0.0322 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "792/792 [==============================] - 3s - loss: 0.1576 - acc: 0.9470 - val_loss: 0.0320 - val_acc: 0.9899\n",
      "Epoch 62/100\n",
      "792/792 [==============================] - 4s - loss: 0.1572 - acc: 0.9432 - val_loss: 0.0527 - val_acc: 0.9697\n",
      "Epoch 63/100\n",
      "792/792 [==============================] - 3s - loss: 0.1941 - acc: 0.9381 - val_loss: 0.0335 - val_acc: 0.9899\n",
      "Epoch 64/100\n",
      "792/792 [==============================] - 4s - loss: 0.1883 - acc: 0.9318 - val_loss: 0.0320 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "792/792 [==============================] - 4s - loss: 0.1687 - acc: 0.9419 - val_loss: 0.0465 - val_acc: 0.9697\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792/792 [==============================] - 4s - loss: 0.2211 - acc: 0.9293 - val_loss: 0.0728 - val_acc: 0.9798\n",
      "Epoch 67/100\n",
      "792/792 [==============================] - 3s - loss: 0.1891 - acc: 0.9306 - val_loss: 0.0561 - val_acc: 0.9798\n",
      "Epoch 68/100\n",
      "792/792 [==============================] - 3s - loss: 0.1692 - acc: 0.9419 - val_loss: 0.0437 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "792/792 [==============================] - 3s - loss: 0.2135 - acc: 0.9318 - val_loss: 0.0403 - val_acc: 0.9899\n",
      "Epoch 70/100\n",
      "792/792 [==============================] - 3s - loss: 0.1820 - acc: 0.9343 - val_loss: 0.0589 - val_acc: 0.9697\n",
      "Epoch 71/100\n",
      "792/792 [==============================] - 3s - loss: 0.1948 - acc: 0.9356 - val_loss: 0.0472 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "792/792 [==============================] - 3s - loss: 0.2062 - acc: 0.9318 - val_loss: 0.0647 - val_acc: 0.9798\n",
      "Epoch 73/100\n",
      "792/792 [==============================] - 4s - loss: 0.1453 - acc: 0.9545 - val_loss: 0.0509 - val_acc: 0.9798\n",
      "Epoch 74/100\n",
      "792/792 [==============================] - 4s - loss: 0.1567 - acc: 0.9457 - val_loss: 0.0423 - val_acc: 0.9899\n",
      "Epoch 75/100\n",
      "792/792 [==============================] - 4s - loss: 0.1678 - acc: 0.9457 - val_loss: 0.0407 - val_acc: 0.9798\n",
      "Epoch 76/100\n",
      "792/792 [==============================] - 4s - loss: 0.1348 - acc: 0.9571 - val_loss: 0.0454 - val_acc: 0.9899\n",
      "Epoch 77/100\n",
      "792/792 [==============================] - 4s - loss: 0.1880 - acc: 0.9343 - val_loss: 0.0348 - val_acc: 0.9899\n",
      "Epoch 78/100\n",
      "792/792 [==============================] - 4s - loss: 0.1851 - acc: 0.9280 - val_loss: 0.0582 - val_acc: 0.9798\n",
      "Epoch 79/100\n",
      "792/792 [==============================] - 3s - loss: 0.1820 - acc: 0.9419 - val_loss: 0.0717 - val_acc: 0.9697\n",
      "Epoch 80/100\n",
      "792/792 [==============================] - 4s - loss: 0.1619 - acc: 0.9444 - val_loss: 0.1002 - val_acc: 0.9596\n",
      "Epoch 81/100\n",
      "792/792 [==============================] - 4s - loss: 0.1631 - acc: 0.9457 - val_loss: 0.0722 - val_acc: 0.9697\n",
      "Epoch 82/100\n",
      "792/792 [==============================] - 4s - loss: 0.2316 - acc: 0.9154 - val_loss: 0.0384 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "792/792 [==============================] - 3s - loss: 0.1779 - acc: 0.9407 - val_loss: 0.0498 - val_acc: 0.9798\n",
      "Epoch 84/100\n",
      "792/792 [==============================] - 3s - loss: 0.1905 - acc: 0.9318 - val_loss: 0.0637 - val_acc: 0.9798\n",
      "Epoch 85/100\n",
      "792/792 [==============================] - 3s - loss: 0.1543 - acc: 0.9407 - val_loss: 0.1025 - val_acc: 0.9798\n",
      "Epoch 86/100\n",
      "792/792 [==============================] - 3s - loss: 0.1790 - acc: 0.9369 - val_loss: 0.1353 - val_acc: 0.9596\n",
      "Epoch 87/100\n",
      "792/792 [==============================] - 3s - loss: 0.1904 - acc: 0.9230 - val_loss: 0.0668 - val_acc: 0.9697\n",
      "Epoch 88/100\n",
      "792/792 [==============================] - 3s - loss: 0.1368 - acc: 0.9508 - val_loss: 0.0465 - val_acc: 0.9798\n",
      "Epoch 89/100\n",
      "792/792 [==============================] - 3s - loss: 0.1679 - acc: 0.9432 - val_loss: 0.0843 - val_acc: 0.9697\n",
      "Epoch 90/100\n",
      "792/792 [==============================] - 3s - loss: 0.1639 - acc: 0.9394 - val_loss: 0.0325 - val_acc: 0.9798\n",
      "Epoch 91/100\n",
      "792/792 [==============================] - 3s - loss: 0.1517 - acc: 0.9520 - val_loss: 0.0543 - val_acc: 0.9697\n",
      "Epoch 92/100\n",
      "792/792 [==============================] - 3s - loss: 0.1491 - acc: 0.9457 - val_loss: 0.0376 - val_acc: 0.9798\n",
      "Epoch 93/100\n",
      "792/792 [==============================] - 3s - loss: 0.1304 - acc: 0.9596 - val_loss: 0.0671 - val_acc: 0.9697\n",
      "Epoch 94/100\n",
      "792/792 [==============================] - 3s - loss: 0.1553 - acc: 0.9394 - val_loss: 0.0462 - val_acc: 0.9697\n",
      "Epoch 95/100\n",
      "792/792 [==============================] - 3s - loss: 0.1491 - acc: 0.9508 - val_loss: 0.0583 - val_acc: 0.9697\n",
      "Epoch 96/100\n",
      "792/792 [==============================] - 3s - loss: 0.1720 - acc: 0.9407 - val_loss: 0.0672 - val_acc: 0.9697\n",
      "Epoch 97/100\n",
      "792/792 [==============================] - 3s - loss: 0.1393 - acc: 0.9545 - val_loss: 0.0458 - val_acc: 0.9798\n",
      "Epoch 98/100\n",
      "792/792 [==============================] - 3s - loss: 0.1470 - acc: 0.9495 - val_loss: 0.0612 - val_acc: 0.9596\n",
      "Epoch 99/100\n",
      "792/792 [==============================] - 3s - loss: 0.1703 - acc: 0.9444 - val_loss: 0.0602 - val_acc: 0.9697\n",
      "Epoch 100/100\n",
      "792/792 [==============================] - 3s - loss: 0.1356 - acc: 0.9508 - val_loss: 0.0477 - val_acc: 0.9697\n"
     ]
    }
   ],
   "source": [
    "history_cat = model.fit([train[1][:],\n",
    "                         train_features],\n",
    "                        train_labels,\n",
    "                        validation_data = ([validation[1][:],\n",
    "                                            validation_features],\n",
    "                                           valid_labels),epochs= 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(history_cat.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.032019013430772708"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(history_cat.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95959596019802673"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(history_cat.history['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13039327217171889"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(history_cat.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f70b6039110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_cat.history['val_acc'])\n",
    "plt.plot(history_cat.history[\"acc\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f70b6194590>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndyKlLbN0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f70bbd2b310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.plot(history_cat.history['loss'])\n",
    "plt.plot(history_cat.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 792 samples, validate on 99 samples\n",
      "Epoch 1/200\n",
      "792/792 [==============================] - 3s - loss: 0.1708 - acc: 0.9470 - val_loss: 0.0648 - val_acc: 0.9899\n",
      "Epoch 2/200\n",
      "792/792 [==============================] - 3s - loss: 0.1407 - acc: 0.9571 - val_loss: 0.0751 - val_acc: 0.9798\n",
      "Epoch 3/200\n",
      "792/792 [==============================] - 3s - loss: 0.1369 - acc: 0.9558 - val_loss: 0.0691 - val_acc: 0.9798\n",
      "Epoch 4/200\n",
      "792/792 [==============================] - 3s - loss: 0.1616 - acc: 0.9495 - val_loss: 0.1728 - val_acc: 0.9495\n",
      "Epoch 5/200\n",
      "792/792 [==============================] - 3s - loss: 0.1768 - acc: 0.9394 - val_loss: 0.0899 - val_acc: 0.9697\n",
      "Epoch 6/200\n",
      "792/792 [==============================] - 4s - loss: 0.1667 - acc: 0.9318 - val_loss: 0.0682 - val_acc: 0.9697\n",
      "Epoch 7/200\n",
      "792/792 [==============================] - 4s - loss: 0.1345 - acc: 0.9508 - val_loss: 0.1104 - val_acc: 0.9596\n",
      "Epoch 8/200\n",
      "792/792 [==============================] - 3s - loss: 0.1473 - acc: 0.9520 - val_loss: 0.1172 - val_acc: 0.9798\n",
      "Epoch 9/200\n",
      "792/792 [==============================] - 4s - loss: 0.1638 - acc: 0.9394 - val_loss: 0.1456 - val_acc: 0.9697\n",
      "Epoch 10/200\n",
      "792/792 [==============================] - 3s - loss: 0.1532 - acc: 0.9432 - val_loss: 0.1236 - val_acc: 0.9596\n",
      "Epoch 11/200\n",
      "792/792 [==============================] - 4s - loss: 0.1311 - acc: 0.9508 - val_loss: 0.1007 - val_acc: 0.9798\n",
      "Epoch 12/200\n",
      "792/792 [==============================] - 4s - loss: 0.1166 - acc: 0.9533 - val_loss: 0.0914 - val_acc: 0.9798\n",
      "Epoch 13/200\n",
      "792/792 [==============================] - 4s - loss: 0.1372 - acc: 0.9583 - val_loss: 0.1159 - val_acc: 0.9596\n",
      "Epoch 14/200\n",
      "792/792 [==============================] - 3s - loss: 0.1784 - acc: 0.9419 - val_loss: 0.1568 - val_acc: 0.9596\n",
      "Epoch 15/200\n",
      "792/792 [==============================] - 3s - loss: 0.1258 - acc: 0.9634 - val_loss: 0.1014 - val_acc: 0.9596\n",
      "Epoch 16/200\n",
      "792/792 [==============================] - 3s - loss: 0.1424 - acc: 0.9520 - val_loss: 0.1159 - val_acc: 0.9697\n",
      "Epoch 17/200\n",
      "792/792 [==============================] - 3s - loss: 0.1428 - acc: 0.9508 - val_loss: 0.1312 - val_acc: 0.9495\n",
      "Epoch 18/200\n",
      "792/792 [==============================] - 3s - loss: 0.1443 - acc: 0.9369 - val_loss: 0.1156 - val_acc: 0.9596\n",
      "Epoch 19/200\n",
      "792/792 [==============================] - 3s - loss: 0.1395 - acc: 0.9495 - val_loss: 0.0892 - val_acc: 0.9596\n",
      "Epoch 20/200\n",
      "792/792 [==============================] - 3s - loss: 0.1342 - acc: 0.9470 - val_loss: 0.0824 - val_acc: 0.9798\n",
      "Epoch 21/200\n",
      "792/792 [==============================] - 4s - loss: 0.1273 - acc: 0.9583 - val_loss: 0.0909 - val_acc: 0.9697\n",
      "Epoch 22/200\n",
      "792/792 [==============================] - 3s - loss: 0.1208 - acc: 0.9545 - val_loss: 0.0870 - val_acc: 0.9697\n",
      "Epoch 23/200\n",
      "792/792 [==============================] - 3s - loss: 0.1203 - acc: 0.9634 - val_loss: 0.0895 - val_acc: 0.9697\n",
      "Epoch 24/200\n",
      "792/792 [==============================] - 3s - loss: 0.1501 - acc: 0.9508 - val_loss: 0.0629 - val_acc: 0.9798\n",
      "Epoch 25/200\n",
      "792/792 [==============================] - 3s - loss: 0.1106 - acc: 0.9596 - val_loss: 0.1161 - val_acc: 0.9596\n",
      "Epoch 26/200\n",
      "792/792 [==============================] - 3s - loss: 0.1332 - acc: 0.9508 - val_loss: 0.0714 - val_acc: 0.9697\n",
      "Epoch 27/200\n",
      "792/792 [==============================] - 3s - loss: 0.1330 - acc: 0.9545 - val_loss: 0.1038 - val_acc: 0.9596\n",
      "Epoch 28/200\n",
      "792/792 [==============================] - 3s - loss: 0.1418 - acc: 0.9545 - val_loss: 0.1575 - val_acc: 0.9596\n",
      "Epoch 29/200\n",
      "792/792 [==============================] - 4s - loss: 0.1082 - acc: 0.9621 - val_loss: 0.1281 - val_acc: 0.9697\n",
      "Epoch 30/200\n",
      "792/792 [==============================] - 3s - loss: 0.1499 - acc: 0.9482 - val_loss: 0.1567 - val_acc: 0.9596\n",
      "Epoch 31/200\n",
      "792/792 [==============================] - 3s - loss: 0.1863 - acc: 0.9331 - val_loss: 0.0906 - val_acc: 0.9798\n",
      "Epoch 32/200\n",
      "792/792 [==============================] - 4s - loss: 0.1469 - acc: 0.9558 - val_loss: 0.1020 - val_acc: 0.9697\n",
      "Epoch 33/200\n",
      "792/792 [==============================] - 4s - loss: 0.1445 - acc: 0.9508 - val_loss: 0.1499 - val_acc: 0.9596\n",
      "Epoch 34/200\n",
      "792/792 [==============================] - 3s - loss: 0.1005 - acc: 0.9659 - val_loss: 0.1029 - val_acc: 0.9798\n",
      "Epoch 35/200\n",
      "792/792 [==============================] - 3s - loss: 0.1146 - acc: 0.9609 - val_loss: 0.0774 - val_acc: 0.9899\n",
      "Epoch 36/200\n",
      "792/792 [==============================] - 3s - loss: 0.1344 - acc: 0.9508 - val_loss: 0.1191 - val_acc: 0.9697\n",
      "Epoch 37/200\n",
      "792/792 [==============================] - 3s - loss: 0.1248 - acc: 0.9558 - val_loss: 0.1176 - val_acc: 0.9697\n",
      "Epoch 38/200\n",
      "792/792 [==============================] - 4s - loss: 0.1192 - acc: 0.9596 - val_loss: 0.1376 - val_acc: 0.9697\n",
      "Epoch 39/200\n",
      "792/792 [==============================] - 3s - loss: 0.1581 - acc: 0.9495 - val_loss: 0.0830 - val_acc: 0.9697\n",
      "Epoch 40/200\n",
      "792/792 [==============================] - 3s - loss: 0.1073 - acc: 0.9634 - val_loss: 0.1064 - val_acc: 0.9596\n",
      "Epoch 41/200\n",
      "792/792 [==============================] - 3s - loss: 0.1465 - acc: 0.9419 - val_loss: 0.1291 - val_acc: 0.9697\n",
      "Epoch 42/200\n",
      "792/792 [==============================] - 3s - loss: 0.0923 - acc: 0.9634 - val_loss: 0.1090 - val_acc: 0.9495\n",
      "Epoch 43/200\n",
      "792/792 [==============================] - 3s - loss: 0.1105 - acc: 0.9609 - val_loss: 0.1430 - val_acc: 0.9394\n",
      "Epoch 44/200\n",
      "792/792 [==============================] - 3s - loss: 0.1235 - acc: 0.9596 - val_loss: 0.1563 - val_acc: 0.9596\n",
      "Epoch 45/200\n",
      "792/792 [==============================] - 4s - loss: 0.1114 - acc: 0.9558 - val_loss: 0.1095 - val_acc: 0.9596\n",
      "Epoch 46/200\n",
      "792/792 [==============================] - 3s - loss: 0.1362 - acc: 0.9558 - val_loss: 0.1339 - val_acc: 0.9495\n",
      "Epoch 47/200\n",
      "792/792 [==============================] - 3s - loss: 0.1057 - acc: 0.9609 - val_loss: 0.1088 - val_acc: 0.9697\n",
      "Epoch 48/200\n",
      "792/792 [==============================] - 3s - loss: 0.1460 - acc: 0.9444 - val_loss: 0.0965 - val_acc: 0.9697\n",
      "Epoch 49/200\n",
      "792/792 [==============================] - 3s - loss: 0.1405 - acc: 0.9520 - val_loss: 0.0873 - val_acc: 0.9798\n",
      "Epoch 50/200\n",
      "792/792 [==============================] - 3s - loss: 0.1116 - acc: 0.9659 - val_loss: 0.1500 - val_acc: 0.9495\n",
      "Epoch 51/200\n",
      "792/792 [==============================] - 3s - loss: 0.1028 - acc: 0.9710 - val_loss: 0.0892 - val_acc: 0.9495\n",
      "Epoch 52/200\n",
      "792/792 [==============================] - 4s - loss: 0.1092 - acc: 0.9634 - val_loss: 0.0857 - val_acc: 0.9798\n",
      "Epoch 53/200\n",
      "792/792 [==============================] - 3s - loss: 0.1021 - acc: 0.9583 - val_loss: 0.0836 - val_acc: 0.9899\n",
      "Epoch 54/200\n",
      "792/792 [==============================] - 3s - loss: 0.1223 - acc: 0.9558 - val_loss: 0.1062 - val_acc: 0.9697\n",
      "Epoch 55/200\n",
      "792/792 [==============================] - 3s - loss: 0.0821 - acc: 0.9735 - val_loss: 0.1246 - val_acc: 0.9596\n",
      "Epoch 56/200\n",
      "792/792 [==============================] - 3s - loss: 0.0988 - acc: 0.9634 - val_loss: 0.1334 - val_acc: 0.9596\n",
      "Epoch 57/200\n",
      "792/792 [==============================] - 3s - loss: 0.0985 - acc: 0.9634 - val_loss: 0.1399 - val_acc: 0.9596\n",
      "Epoch 58/200\n",
      "792/792 [==============================] - 3s - loss: 0.1457 - acc: 0.9508 - val_loss: 0.1188 - val_acc: 0.9495\n",
      "Epoch 59/200\n",
      "792/792 [==============================] - 3s - loss: 0.1044 - acc: 0.9634 - val_loss: 0.1451 - val_acc: 0.9596\n",
      "Epoch 60/200\n",
      "792/792 [==============================] - 3s - loss: 0.1062 - acc: 0.9621 - val_loss: 0.1082 - val_acc: 0.9697\n",
      "Epoch 61/200\n",
      "792/792 [==============================] - 3s - loss: 0.1040 - acc: 0.9672 - val_loss: 0.0900 - val_acc: 0.9596\n",
      "Epoch 62/200\n",
      "792/792 [==============================] - 4s - loss: 0.1168 - acc: 0.9672 - val_loss: 0.0778 - val_acc: 0.9596\n",
      "Epoch 63/200\n",
      "792/792 [==============================] - 3s - loss: 0.1058 - acc: 0.9634 - val_loss: 0.0509 - val_acc: 0.9697\n",
      "Epoch 64/200\n",
      "792/792 [==============================] - 3s - loss: 0.1110 - acc: 0.9659 - val_loss: 0.0966 - val_acc: 0.9596\n",
      "Epoch 65/200\n",
      "792/792 [==============================] - 3s - loss: 0.1288 - acc: 0.9571 - val_loss: 0.0874 - val_acc: 0.9697\n",
      "Epoch 66/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792/792 [==============================] - 3s - loss: 0.1305 - acc: 0.9533 - val_loss: 0.0835 - val_acc: 0.9697\n",
      "Epoch 67/200\n",
      "792/792 [==============================] - 3s - loss: 0.1044 - acc: 0.9697 - val_loss: 0.1065 - val_acc: 0.9495\n",
      "Epoch 68/200\n",
      "792/792 [==============================] - 3s - loss: 0.1017 - acc: 0.9609 - val_loss: 0.0936 - val_acc: 0.9798\n",
      "Epoch 69/200\n",
      "792/792 [==============================] - 3s - loss: 0.0977 - acc: 0.9672 - val_loss: 0.0341 - val_acc: 1.0000\n",
      "Epoch 70/200\n",
      "792/792 [==============================] - 3s - loss: 0.1058 - acc: 0.9533 - val_loss: 0.0843 - val_acc: 0.9697\n",
      "Epoch 71/200\n",
      "792/792 [==============================] - 3s - loss: 0.0882 - acc: 0.9722 - val_loss: 0.0966 - val_acc: 0.9495\n",
      "Epoch 72/200\n",
      "792/792 [==============================] - 3s - loss: 0.0880 - acc: 0.9722 - val_loss: 0.0910 - val_acc: 0.9596\n",
      "Epoch 73/200\n",
      "792/792 [==============================] - 3s - loss: 0.0972 - acc: 0.9609 - val_loss: 0.0434 - val_acc: 0.9798\n",
      "Epoch 74/200\n",
      "792/792 [==============================] - 3s - loss: 0.0873 - acc: 0.9722 - val_loss: 0.0465 - val_acc: 0.9798\n",
      "Epoch 75/200\n",
      "792/792 [==============================] - 4s - loss: 0.1226 - acc: 0.9571 - val_loss: 0.0785 - val_acc: 0.9697\n",
      "Epoch 76/200\n",
      "792/792 [==============================] - 4s - loss: 0.1175 - acc: 0.9621 - val_loss: 0.0905 - val_acc: 0.9697\n",
      "Epoch 77/200\n",
      "792/792 [==============================] - 3s - loss: 0.1089 - acc: 0.9634 - val_loss: 0.0747 - val_acc: 0.9899\n",
      "Epoch 78/200\n",
      "792/792 [==============================] - 3s - loss: 0.0831 - acc: 0.9747 - val_loss: 0.0746 - val_acc: 0.9697\n",
      "Epoch 79/200\n",
      "792/792 [==============================] - 3s - loss: 0.1212 - acc: 0.9558 - val_loss: 0.0802 - val_acc: 0.9697\n",
      "Epoch 80/200\n",
      "792/792 [==============================] - 3s - loss: 0.1004 - acc: 0.9646 - val_loss: 0.0735 - val_acc: 0.9697\n",
      "Epoch 81/200\n",
      "792/792 [==============================] - 3s - loss: 0.1382 - acc: 0.9558 - val_loss: 0.0628 - val_acc: 0.9798\n",
      "Epoch 82/200\n",
      "792/792 [==============================] - 3s - loss: 0.1248 - acc: 0.9545 - val_loss: 0.0718 - val_acc: 0.9697\n",
      "Epoch 83/200\n",
      "792/792 [==============================] - 3s - loss: 0.0934 - acc: 0.9722 - val_loss: 0.0691 - val_acc: 0.9798\n",
      "Epoch 84/200\n",
      "792/792 [==============================] - 3s - loss: 0.1451 - acc: 0.9545 - val_loss: 0.0768 - val_acc: 0.9697\n",
      "Epoch 85/200\n",
      "792/792 [==============================] - 3s - loss: 0.0971 - acc: 0.9672 - val_loss: 0.0521 - val_acc: 0.9798\n",
      "Epoch 86/200\n",
      "792/792 [==============================] - 3s - loss: 0.0804 - acc: 0.9684 - val_loss: 0.0599 - val_acc: 0.9697\n",
      "Epoch 87/200\n",
      "792/792 [==============================] - 3s - loss: 0.0751 - acc: 0.9747 - val_loss: 0.0429 - val_acc: 0.9798\n",
      "Epoch 88/200\n",
      "792/792 [==============================] - 4s - loss: 0.0973 - acc: 0.9710 - val_loss: 0.0566 - val_acc: 0.9899\n",
      "Epoch 89/200\n",
      "792/792 [==============================] - 3s - loss: 0.0918 - acc: 0.9659 - val_loss: 0.0573 - val_acc: 0.9899\n",
      "Epoch 90/200\n",
      "792/792 [==============================] - 3s - loss: 0.0652 - acc: 0.9760 - val_loss: 0.0517 - val_acc: 0.9899\n",
      "Epoch 91/200\n",
      "792/792 [==============================] - 3s - loss: 0.0863 - acc: 0.9697 - val_loss: 0.0547 - val_acc: 0.9798\n",
      "Epoch 92/200\n",
      "792/792 [==============================] - 4s - loss: 0.0977 - acc: 0.9659 - val_loss: 0.0427 - val_acc: 0.9697\n",
      "Epoch 93/200\n",
      "792/792 [==============================] - 4s - loss: 0.1039 - acc: 0.9659 - val_loss: 0.0551 - val_acc: 0.9697\n",
      "Epoch 94/200\n",
      "792/792 [==============================] - 3s - loss: 0.0857 - acc: 0.9646 - val_loss: 0.0635 - val_acc: 0.9697\n",
      "Epoch 95/200\n",
      "792/792 [==============================] - 3s - loss: 0.0864 - acc: 0.9735 - val_loss: 0.0676 - val_acc: 0.9697\n",
      "Epoch 96/200\n",
      "792/792 [==============================] - 3s - loss: 0.0906 - acc: 0.9621 - val_loss: 0.0944 - val_acc: 0.9495\n",
      "Epoch 97/200\n",
      "792/792 [==============================] - 4s - loss: 0.1178 - acc: 0.9659 - val_loss: 0.0733 - val_acc: 0.9798\n",
      "Epoch 98/200\n",
      "792/792 [==============================] - 4s - loss: 0.0838 - acc: 0.9722 - val_loss: 0.0997 - val_acc: 0.9596\n",
      "Epoch 99/200\n",
      "792/792 [==============================] - 4s - loss: 0.0976 - acc: 0.9735 - val_loss: 0.0735 - val_acc: 0.9798\n",
      "Epoch 100/200\n",
      "792/792 [==============================] - 3s - loss: 0.0945 - acc: 0.9646 - val_loss: 0.1049 - val_acc: 0.9798\n",
      "Epoch 101/200\n",
      "792/792 [==============================] - 3s - loss: 0.0817 - acc: 0.9710 - val_loss: 0.0484 - val_acc: 0.9798\n",
      "Epoch 102/200\n",
      "792/792 [==============================] - 3s - loss: 0.0854 - acc: 0.9710 - val_loss: 0.0727 - val_acc: 0.9697\n",
      "Epoch 103/200\n",
      "792/792 [==============================] - 4s - loss: 0.1258 - acc: 0.9533 - val_loss: 0.0667 - val_acc: 0.9697\n",
      "Epoch 104/200\n",
      "792/792 [==============================] - 3s - loss: 0.0624 - acc: 0.9798 - val_loss: 0.1651 - val_acc: 0.9293\n",
      "Epoch 105/200\n",
      "792/792 [==============================] - 3s - loss: 0.0811 - acc: 0.9697 - val_loss: 0.1250 - val_acc: 0.9495\n",
      "Epoch 106/200\n",
      "792/792 [==============================] - 3s - loss: 0.0878 - acc: 0.9697 - val_loss: 0.0927 - val_acc: 0.9697\n",
      "Epoch 107/200\n",
      "792/792 [==============================] - 3s - loss: 0.0923 - acc: 0.9684 - val_loss: 0.0703 - val_acc: 0.9899\n",
      "Epoch 108/200\n",
      "792/792 [==============================] - 3s - loss: 0.0951 - acc: 0.9710 - val_loss: 0.1329 - val_acc: 0.9596\n",
      "Epoch 109/200\n",
      "792/792 [==============================] - 3s - loss: 0.1180 - acc: 0.9634 - val_loss: 0.0959 - val_acc: 0.9596\n",
      "Epoch 110/200\n",
      "792/792 [==============================] - 3s - loss: 0.0944 - acc: 0.9710 - val_loss: 0.1154 - val_acc: 0.9495\n",
      "Epoch 111/200\n",
      "792/792 [==============================] - 3s - loss: 0.0910 - acc: 0.9659 - val_loss: 0.1304 - val_acc: 0.9596\n",
      "Epoch 112/200\n",
      "792/792 [==============================] - 3s - loss: 0.0701 - acc: 0.9735 - val_loss: 0.1441 - val_acc: 0.9495\n",
      "Epoch 113/200\n",
      "792/792 [==============================] - 3s - loss: 0.1068 - acc: 0.9583 - val_loss: 0.1554 - val_acc: 0.9394\n",
      "Epoch 114/200\n",
      "792/792 [==============================] - 3s - loss: 0.0935 - acc: 0.9646 - val_loss: 0.0948 - val_acc: 0.9596\n",
      "Epoch 115/200\n",
      "792/792 [==============================] - 4s - loss: 0.1119 - acc: 0.9558 - val_loss: 0.1488 - val_acc: 0.9293\n",
      "Epoch 116/200\n",
      "792/792 [==============================] - 3s - loss: 0.0653 - acc: 0.9722 - val_loss: 0.1403 - val_acc: 0.9596\n",
      "Epoch 117/200\n",
      "792/792 [==============================] - 3s - loss: 0.0893 - acc: 0.9697 - val_loss: 0.1361 - val_acc: 0.9293\n",
      "Epoch 118/200\n",
      "792/792 [==============================] - 3s - loss: 0.0839 - acc: 0.9697 - val_loss: 0.1255 - val_acc: 0.9394\n",
      "Epoch 119/200\n",
      "792/792 [==============================] - 3s - loss: 0.0751 - acc: 0.9672 - val_loss: 0.0690 - val_acc: 0.9596\n",
      "Epoch 120/200\n",
      "792/792 [==============================] - 3s - loss: 0.0799 - acc: 0.9722 - val_loss: 0.0990 - val_acc: 0.9596\n",
      "Epoch 121/200\n",
      "792/792 [==============================] - 4s - loss: 0.1010 - acc: 0.9684 - val_loss: 0.1135 - val_acc: 0.9697\n",
      "Epoch 122/200\n",
      "792/792 [==============================] - 4s - loss: 0.1008 - acc: 0.9672 - val_loss: 0.1416 - val_acc: 0.9495\n",
      "Epoch 123/200\n",
      "792/792 [==============================] - 4s - loss: 0.0738 - acc: 0.9722 - val_loss: 0.1531 - val_acc: 0.9394\n",
      "Epoch 124/200\n",
      "792/792 [==============================] - 3s - loss: 0.0984 - acc: 0.9646 - val_loss: 0.1456 - val_acc: 0.9495\n",
      "Epoch 125/200\n",
      "792/792 [==============================] - 4s - loss: 0.0982 - acc: 0.9659 - val_loss: 0.1349 - val_acc: 0.9697\n",
      "Epoch 126/200\n",
      "792/792 [==============================] - 3s - loss: 0.0694 - acc: 0.9735 - val_loss: 0.1664 - val_acc: 0.9596\n",
      "Epoch 127/200\n",
      "792/792 [==============================] - 4s - loss: 0.0794 - acc: 0.9735 - val_loss: 0.1436 - val_acc: 0.9697\n",
      "Epoch 128/200\n",
      "792/792 [==============================] - 3s - loss: 0.1256 - acc: 0.9520 - val_loss: 0.1730 - val_acc: 0.9596\n",
      "Epoch 129/200\n",
      "792/792 [==============================] - 4s - loss: 0.0858 - acc: 0.9659 - val_loss: 0.1729 - val_acc: 0.9495\n",
      "Epoch 130/200\n",
      "792/792 [==============================] - 3s - loss: 0.1095 - acc: 0.9621 - val_loss: 0.0977 - val_acc: 0.9798\n",
      "Epoch 131/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792/792 [==============================] - 4s - loss: 0.0957 - acc: 0.9697 - val_loss: 0.1377 - val_acc: 0.9596\n",
      "Epoch 132/200\n",
      "792/792 [==============================] - 4s - loss: 0.0611 - acc: 0.9785 - val_loss: 0.1364 - val_acc: 0.9596\n",
      "Epoch 133/200\n",
      "792/792 [==============================] - 4s - loss: 0.0866 - acc: 0.9684 - val_loss: 0.1317 - val_acc: 0.9697\n",
      "Epoch 134/200\n",
      "792/792 [==============================] - 3s - loss: 0.0858 - acc: 0.9722 - val_loss: 0.1361 - val_acc: 0.9697\n",
      "Epoch 135/200\n",
      "792/792 [==============================] - 3s - loss: 0.0971 - acc: 0.9722 - val_loss: 0.1487 - val_acc: 0.9495\n",
      "Epoch 136/200\n",
      "792/792 [==============================] - 4s - loss: 0.1147 - acc: 0.9621 - val_loss: 0.1345 - val_acc: 0.9596\n",
      "Epoch 137/200\n",
      "792/792 [==============================] - 3s - loss: 0.1072 - acc: 0.9596 - val_loss: 0.1146 - val_acc: 0.9697\n",
      "Epoch 138/200\n",
      "792/792 [==============================] - 3s - loss: 0.1110 - acc: 0.9609 - val_loss: 0.1296 - val_acc: 0.9596\n",
      "Epoch 139/200\n",
      "792/792 [==============================] - 3s - loss: 0.0779 - acc: 0.9672 - val_loss: 0.1090 - val_acc: 0.9697\n",
      "Epoch 140/200\n",
      "792/792 [==============================] - 3s - loss: 0.0804 - acc: 0.9747 - val_loss: 0.1120 - val_acc: 0.9596\n",
      "Epoch 141/200\n",
      "792/792 [==============================] - 3s - loss: 0.0619 - acc: 0.9773 - val_loss: 0.0992 - val_acc: 0.9596\n",
      "Epoch 142/200\n",
      "792/792 [==============================] - 3s - loss: 0.1194 - acc: 0.9596 - val_loss: 0.0668 - val_acc: 0.9798\n",
      "Epoch 143/200\n",
      "792/792 [==============================] - 4s - loss: 0.0715 - acc: 0.9747 - val_loss: 0.0702 - val_acc: 0.9697\n",
      "Epoch 144/200\n",
      "792/792 [==============================] - 3s - loss: 0.1011 - acc: 0.9672 - val_loss: 0.0515 - val_acc: 0.9798\n",
      "Epoch 145/200\n",
      "792/792 [==============================] - 3s - loss: 0.0738 - acc: 0.9735 - val_loss: 0.0736 - val_acc: 0.9697\n",
      "Epoch 146/200\n",
      "792/792 [==============================] - 4s - loss: 0.0885 - acc: 0.9672 - val_loss: 0.0643 - val_acc: 0.9798\n",
      "Epoch 147/200\n",
      "792/792 [==============================] - 3s - loss: 0.0902 - acc: 0.9634 - val_loss: 0.0916 - val_acc: 0.9495\n",
      "Epoch 148/200\n",
      "792/792 [==============================] - 4s - loss: 0.0648 - acc: 0.9773 - val_loss: 0.0548 - val_acc: 0.9697\n",
      "Epoch 149/200\n",
      "792/792 [==============================] - 4s - loss: 0.0637 - acc: 0.9722 - val_loss: 0.0764 - val_acc: 0.9697\n",
      "Epoch 150/200\n",
      "792/792 [==============================] - 3s - loss: 0.1089 - acc: 0.9609 - val_loss: 0.0651 - val_acc: 0.9697\n",
      "Epoch 151/200\n",
      "792/792 [==============================] - 3s - loss: 0.0793 - acc: 0.9710 - val_loss: 0.0581 - val_acc: 0.9798\n",
      "Epoch 152/200\n",
      "792/792 [==============================] - 4s - loss: 0.0790 - acc: 0.9672 - val_loss: 0.0629 - val_acc: 0.9899\n",
      "Epoch 153/200\n",
      "792/792 [==============================] - 4s - loss: 0.0910 - acc: 0.9760 - val_loss: 0.0706 - val_acc: 0.9697\n",
      "Epoch 154/200\n",
      "792/792 [==============================] - 3s - loss: 0.0578 - acc: 0.9785 - val_loss: 0.0609 - val_acc: 0.9697\n",
      "Epoch 155/200\n",
      "792/792 [==============================] - 3s - loss: 0.0688 - acc: 0.9785 - val_loss: 0.0936 - val_acc: 0.9596\n",
      "Epoch 156/200\n",
      "792/792 [==============================] - 4s - loss: 0.0877 - acc: 0.9697 - val_loss: 0.1005 - val_acc: 0.9596\n",
      "Epoch 157/200\n",
      "792/792 [==============================] - 4s - loss: 0.1000 - acc: 0.9659 - val_loss: 0.1037 - val_acc: 0.9697\n",
      "Epoch 158/200\n",
      "792/792 [==============================] - 4s - loss: 0.0759 - acc: 0.9747 - val_loss: 0.0982 - val_acc: 0.9798\n",
      "Epoch 159/200\n",
      "792/792 [==============================] - 3s - loss: 0.0787 - acc: 0.9735 - val_loss: 0.1029 - val_acc: 0.9596\n",
      "Epoch 160/200\n",
      "792/792 [==============================] - 3s - loss: 0.0559 - acc: 0.9874 - val_loss: 0.0776 - val_acc: 0.9697\n",
      "Epoch 161/200\n",
      "792/792 [==============================] - 4s - loss: 0.0835 - acc: 0.9697 - val_loss: 0.0578 - val_acc: 0.9798\n",
      "Epoch 162/200\n",
      "792/792 [==============================] - 3s - loss: 0.0850 - acc: 0.9735 - val_loss: 0.0767 - val_acc: 0.9697\n",
      "Epoch 163/200\n",
      "792/792 [==============================] - 4s - loss: 0.0717 - acc: 0.9760 - val_loss: 0.0663 - val_acc: 0.9697\n",
      "Epoch 164/200\n",
      "792/792 [==============================] - 3s - loss: 0.0830 - acc: 0.9684 - val_loss: 0.0602 - val_acc: 0.9798\n",
      "Epoch 165/200\n",
      "792/792 [==============================] - 3s - loss: 0.0956 - acc: 0.9697 - val_loss: 0.0673 - val_acc: 0.9798\n",
      "Epoch 166/200\n",
      "792/792 [==============================] - 4s - loss: 0.0535 - acc: 0.9798 - val_loss: 0.0955 - val_acc: 0.9697\n",
      "Epoch 167/200\n",
      "792/792 [==============================] - 3s - loss: 0.0707 - acc: 0.9798 - val_loss: 0.1111 - val_acc: 0.9596\n",
      "Epoch 168/200\n",
      "792/792 [==============================] - 4s - loss: 0.0836 - acc: 0.9659 - val_loss: 0.0670 - val_acc: 0.9697\n",
      "Epoch 169/200\n",
      "792/792 [==============================] - 3s - loss: 0.1056 - acc: 0.9722 - val_loss: 0.0748 - val_acc: 0.9697\n",
      "Epoch 170/200\n",
      "792/792 [==============================] - 3s - loss: 0.0750 - acc: 0.9760 - val_loss: 0.0727 - val_acc: 0.9697\n",
      "Epoch 171/200\n",
      "792/792 [==============================] - 4s - loss: 0.0914 - acc: 0.9735 - val_loss: 0.0696 - val_acc: 0.9798\n",
      "Epoch 172/200\n",
      "792/792 [==============================] - 4s - loss: 0.0594 - acc: 0.9773 - val_loss: 0.0675 - val_acc: 0.9697\n",
      "Epoch 173/200\n",
      "792/792 [==============================] - 3s - loss: 0.0732 - acc: 0.9760 - val_loss: 0.0850 - val_acc: 0.9697\n",
      "Epoch 174/200\n",
      "792/792 [==============================] - 4s - loss: 0.0568 - acc: 0.9760 - val_loss: 0.0810 - val_acc: 0.9495\n",
      "Epoch 175/200\n",
      "792/792 [==============================] - 3s - loss: 0.0709 - acc: 0.9722 - val_loss: 0.0701 - val_acc: 0.9697\n",
      "Epoch 176/200\n",
      "792/792 [==============================] - 3s - loss: 0.0929 - acc: 0.9697 - val_loss: 0.1009 - val_acc: 0.9495\n",
      "Epoch 177/200\n",
      "792/792 [==============================] - 4s - loss: 0.0662 - acc: 0.9747 - val_loss: 0.0790 - val_acc: 0.9596\n",
      "Epoch 178/200\n",
      "792/792 [==============================] - 4s - loss: 0.1074 - acc: 0.9621 - val_loss: 0.0707 - val_acc: 0.9697\n",
      "Epoch 179/200\n",
      "792/792 [==============================] - 4s - loss: 0.0745 - acc: 0.9773 - val_loss: 0.0520 - val_acc: 0.9798\n",
      "Epoch 180/200\n",
      "792/792 [==============================] - 3s - loss: 0.0648 - acc: 0.9747 - val_loss: 0.0538 - val_acc: 0.9798\n",
      "Epoch 181/200\n",
      "792/792 [==============================] - 4s - loss: 0.0754 - acc: 0.9747 - val_loss: 0.0639 - val_acc: 0.9697\n",
      "Epoch 182/200\n",
      "792/792 [==============================] - 4s - loss: 0.0915 - acc: 0.9646 - val_loss: 0.0344 - val_acc: 0.9899\n",
      "Epoch 183/200\n",
      "792/792 [==============================] - 4s - loss: 0.1204 - acc: 0.9583 - val_loss: 0.0310 - val_acc: 0.9798\n",
      "Epoch 184/200\n",
      "792/792 [==============================] - 3s - loss: 0.0946 - acc: 0.9735 - val_loss: 0.0500 - val_acc: 0.9798\n",
      "Epoch 185/200\n",
      "792/792 [==============================] - 4s - loss: 0.0846 - acc: 0.9684 - val_loss: 0.0395 - val_acc: 0.9798\n",
      "Epoch 186/200\n",
      "792/792 [==============================] - 3s - loss: 0.0724 - acc: 0.9722 - val_loss: 0.0628 - val_acc: 0.9697\n",
      "Epoch 187/200\n",
      "792/792 [==============================] - 4s - loss: 0.0790 - acc: 0.9773 - val_loss: 0.0606 - val_acc: 0.9798\n",
      "Epoch 188/200\n",
      "792/792 [==============================] - 4s - loss: 0.0660 - acc: 0.9710 - val_loss: 0.0650 - val_acc: 0.9798\n",
      "Epoch 189/200\n",
      "792/792 [==============================] - 4s - loss: 0.0852 - acc: 0.9722 - val_loss: 0.0859 - val_acc: 0.9596\n",
      "Epoch 190/200\n",
      "792/792 [==============================] - 3s - loss: 0.0812 - acc: 0.9710 - val_loss: 0.0638 - val_acc: 0.9798\n",
      "Epoch 191/200\n",
      "792/792 [==============================] - 3s - loss: 0.0733 - acc: 0.9773 - val_loss: 0.0854 - val_acc: 0.9697\n",
      "Epoch 192/200\n",
      "792/792 [==============================] - 3s - loss: 0.0542 - acc: 0.9747 - val_loss: 0.0870 - val_acc: 0.9697\n",
      "Epoch 193/200\n",
      "792/792 [==============================] - 3s - loss: 0.0698 - acc: 0.9773 - val_loss: 0.0622 - val_acc: 0.9798\n",
      "Epoch 194/200\n",
      "792/792 [==============================] - 4s - loss: 0.0702 - acc: 0.9760 - val_loss: 0.0736 - val_acc: 0.9798\n",
      "Epoch 195/200\n",
      "792/792 [==============================] - 3s - loss: 0.0655 - acc: 0.9785 - val_loss: 0.0553 - val_acc: 0.9697\n",
      "Epoch 196/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792/792 [==============================] - 4s - loss: 0.1040 - acc: 0.9672 - val_loss: 0.0667 - val_acc: 0.9697\n",
      "Epoch 197/200\n",
      "792/792 [==============================] - 4s - loss: 0.0810 - acc: 0.9785 - val_loss: 0.0764 - val_acc: 0.9697\n",
      "Epoch 198/200\n",
      "792/792 [==============================] - 3s - loss: 0.0552 - acc: 0.9823 - val_loss: 0.1055 - val_acc: 0.9596\n",
      "Epoch 199/200\n",
      "792/792 [==============================] - 3s - loss: 0.0742 - acc: 0.9735 - val_loss: 0.1032 - val_acc: 0.9697\n",
      "Epoch 200/200\n",
      "792/792 [==============================] - 3s - loss: 0.0812 - acc: 0.9760 - val_loss: 0.0858 - val_acc: 0.9798\n"
     ]
    }
   ],
   "source": [
    "history_cat_2 = model.fit([train[1][:],\n",
    "                         train_features],\n",
    "                        train_labels,\n",
    "                        validation_data = ([validation[1][:],\n",
    "                                            validation_features],\n",
    "                                           valid_labels),epochs= 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Sequential Convolutional Neural Network(CNN)-images only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#start with the images only.\n",
    "\n",
    "cnn = Sequential([\n",
    "    Conv2D(8, kernel_size=(3,3),\n",
    "          activation= \"relu\",\n",
    "          input_shape = (64,64,1)),\n",
    "    Conv2D(16, kernel_size=(3,3), \n",
    "           activation = \"relu\"),\n",
    "    Dropout(.25),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(100, activation = \"relu\"),\n",
    "    Dropout(.25),\n",
    "    #output _ layer\n",
    "    Dense(256),\n",
    "    Dense(99, activation= \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_23 (Conv2D)           (None, 62, 62, 8)         80        \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 60, 60, 16)        1168      \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 60, 60, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 14400)             0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 100)               1440100   \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 99)                25443     \n",
      "=================================================================\n",
      "Total params: 1,492,647\n",
      "Trainable params: 1,492,647\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn.compile(loss= keras.losses.categorical_crossentropy,\n",
    "           optimizer= keras.optimizers.adam(),\n",
    "           metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 792 samples, validate on 99 samples\n",
      "Epoch 1/20\n",
      "792/792 [==============================] - 3s - loss: 4.5576 - acc: 0.0202 - val_loss: 4.2439 - val_acc: 0.0808\n",
      "Epoch 2/20\n",
      "792/792 [==============================] - 3s - loss: 3.7604 - acc: 0.0846 - val_loss: 3.1458 - val_acc: 0.1818\n",
      "Epoch 3/20\n",
      "792/792 [==============================] - 3s - loss: 2.8101 - acc: 0.2449 - val_loss: 2.4668 - val_acc: 0.3535\n",
      "Epoch 4/20\n",
      "792/792 [==============================] - 3s - loss: 2.2514 - acc: 0.3245 - val_loss: 2.1048 - val_acc: 0.4242\n",
      "Epoch 5/20\n",
      "792/792 [==============================] - 3s - loss: 1.8565 - acc: 0.4571 - val_loss: 1.8336 - val_acc: 0.5152\n",
      "Epoch 6/20\n",
      "792/792 [==============================] - 3s - loss: 1.7217 - acc: 0.4722 - val_loss: 1.7084 - val_acc: 0.5354\n",
      "Epoch 7/20\n",
      "792/792 [==============================] - 3s - loss: 1.4511 - acc: 0.5682 - val_loss: 1.5166 - val_acc: 0.5354\n",
      "Epoch 8/20\n",
      "792/792 [==============================] - 3s - loss: 1.2963 - acc: 0.5960 - val_loss: 1.5006 - val_acc: 0.5758\n",
      "Epoch 9/20\n",
      "792/792 [==============================] - 3s - loss: 1.1555 - acc: 0.6288 - val_loss: 1.5559 - val_acc: 0.5253\n",
      "Epoch 10/20\n",
      "792/792 [==============================] - 3s - loss: 1.0568 - acc: 0.6641 - val_loss: 1.5516 - val_acc: 0.5657\n",
      "Epoch 11/20\n",
      "792/792 [==============================] - 3s - loss: 0.9591 - acc: 0.6667 - val_loss: 1.5054 - val_acc: 0.5859\n",
      "Epoch 12/20\n",
      "792/792 [==============================] - 3s - loss: 0.9013 - acc: 0.7247 - val_loss: 1.4944 - val_acc: 0.5859\n",
      "Epoch 13/20\n",
      "792/792 [==============================] - 3s - loss: 0.8083 - acc: 0.7348 - val_loss: 1.4965 - val_acc: 0.5657\n",
      "Epoch 14/20\n",
      "792/792 [==============================] - 3s - loss: 0.7669 - acc: 0.7614 - val_loss: 1.4547 - val_acc: 0.5859\n",
      "Epoch 15/20\n",
      "792/792 [==============================] - 3s - loss: 0.6599 - acc: 0.7929 - val_loss: 1.5280 - val_acc: 0.5859\n",
      "Epoch 16/20\n",
      "792/792 [==============================] - 3s - loss: 0.6646 - acc: 0.7740 - val_loss: 1.4396 - val_acc: 0.5758\n",
      "Epoch 17/20\n",
      "792/792 [==============================] - 3s - loss: 0.6071 - acc: 0.7942 - val_loss: 1.4575 - val_acc: 0.5556\n",
      "Epoch 18/20\n",
      "792/792 [==============================] - 3s - loss: 0.5863 - acc: 0.8144 - val_loss: 1.5812 - val_acc: 0.5556\n",
      "Epoch 19/20\n",
      "792/792 [==============================] - 3s - loss: 0.5764 - acc: 0.7980 - val_loss: 1.7092 - val_acc: 0.5455\n",
      "Epoch 20/20\n",
      "792/792 [==============================] - 3s - loss: 0.4433 - acc: 0.8548 - val_loss: 1.6206 - val_acc: 0.5354\n"
     ]
    }
   ],
   "source": [
    "# for images only ==> train[1] and validation[1]\n",
    "history = cnn.fit(train[1],train_labels, validation_data=(validation[1], valid_labels),nb_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(792, 64, 64, 1)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[1][:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(792, 192)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#you need an optimizer anyway.\n",
    "#sgd = SGD(lr=0.008, momentum=0.9,decay=0.0,nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=[<tf.Tenso...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# for image + numerical data\n",
    "model = Model(input=[input_layer,num],output=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.compile(optimizer='Adam',loss='categorical_crossentropy',\n",
    "#              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install seaborn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
